# Key questions to ask

| Who: Procurement, project, and technology teams, as well as leadership What: Questions to ask during the GenAI procurement processes |
| :---- |

These questions can help your organization make informed decisions and implement GenAI solutions responsibly and effectively. Successful GenAI procurement is a team effort, so as a procurement official, you are unlikely to have all the answers. However, understanding generally what questions need to be asked, and when, can help you facilitate internal conversations and ensure that the right experts are involved in decision-making. 

The questions have been organized by the stage in the procurement process, and who may be best positioned to lead the inquiry. As you review these questions, consider your own unique organizational setup and who in your organization might be best equipped to provide insights on each topic. At different moments, you may need to consult IT, legal, data science, project management, or business units to get the answers you need. 

Not all questions will be relevant for your context. How you use this question sheet will also depend on the contract model that you pursue. For example, if you are buying off-the-shelf GenAI service access, procurement will mostly focus on aspects like licensing and access management, while if you are building or customizing software, the procurement team will work more closely with the project leaders and IT to tackle more complex issues relating to the vendor relationship and technical solution. 

| Building AI-readiness   Objective: lay the groundwork for overall good GenAI procurement through policies and practices |  |
| :---- | :---- |
| **Team** | **Key questions to ask** |
| Procurement | Can we use agile methods for our IT procurements? How can our procurement processes support strong collaboration between technical teams, procurement, legal, and policy stakeholders? |
| Agency or department buyer | How do our overall organizational mission and goals align with our AI initiatives?  What are our internal capability gaps? |
| IT/Data science | Who inside the organization can take the lead on GenAI and connect the dots across teams? What governance structures and frameworks do we need for responsible and ethical GenAI adoption? Do we have the necessary AI literacy and understanding across relevant stakeholders? How will we manage internal change and encourage safe adoption of GenAI tools? How will we ensure inclusive design and testing of GenAI tools to reflect diverse public needs? |

| Project design Objective: identify your needs for your specific procurement |  |
| :---- | :---- |
| **Team** | **Key questions to ask** |
| Agency or department buyer | **Model and system architecture** What is the problem we are trying to solve?  What are our needs and use cases?   What are the expected outcomes of the solution we want to acquire?  How might we safely test our assumptions, approach, and GenAI tool options before launching the formal procurement process?   **Operational considerations** Which role or team will be accountable for managing risk and compliance? Do we have the internal capacity for this oversight? |
| IT/Data science | **Model and system architecture** Is GenAI the right tool to meet our needsW? What deterministic and rule-based solutions might deliver comparable results to generative AI for our use case? If we pursue GenAI, what model architecture best suits our specific use case needs (e.g., general-purpose vs. specialized models)? What integration capabilities do we need with existing systems? What is our internal capacity to maintain the tool? Where will the tool be hosted? Will we need to acquire additional processing or storage capacity? |

| Procurement Objectives:  Decide on your procurement procedure  Draft your solicitation  Evaluate proposals Negotiate and award the contract  |  |
| :---- | :---- |
| **Team** | **Key questions to ask** |
| Procurement | **Vendor & Ecosystem Strategy** How might we reflect the project team’s needs in a RFx?  Can the vendor demonstrate a roadmap and flexibility that aligns with our evolving needs? What level of vendor support or service guarantees will we need if something breaks or goes wrong? Will the vendor support integration with other GenAI tools or models we may adopt in the future? How much risk does vendor lock-in actually pose in our context, and what mitigations are practical? **Operational Considerations** How will we manage and track AI-specific costs (e.g., compute, storage, API usage)? Are the right people in the room for this procurement? **Risk and compliance** How will we ensure compliance with current and evolving AI-related legislation, procurement rules, and standards? |
| Agency or department buyer | **Model and system architecture** Do we know what 'good enough' model response performance looks like for our specific use case, and what metrics will we use to measure GenAI success and ROI (qualitative and quantitative)? What are our fallback strategies if the model gives low-quality or inaccurate responses? How will users interact with the GenAI solution? If there is a platform or dashboard, who is responsible for developing it?  **Data strategy and management** Do we and our vendor partners understand the quality, completeness, and relevance of our data? What role will the vendor play in helping us prepare or transform data for effective use? How will we manage data ownership, licensing, and usage rights?  What safeguards will the vendor provide to ensure that sensitive, personal, or classified data is handled in line with legal and policy requirements?  Will the GenAI system retain or learn from our data inputs? Under what terms, and with what controls? Is the algorithm specialized for our organization? If so, will we have input into how the algorithm is fine-tuned or retrained using our data, and what additional data sources do we need to get the best results?  Can we retrieve or export our data and models if we switch vendors? **Operational considerations** How might we approach GenAI implementation in stages, such as through a pilot or sandbox? What policies will govern human-in-the-loop oversight for critical decision-making? What mechanisms will we put in place for continuous performance monitoring and system health checks? If we are involved in retraining or fine-tuning the model, what internal processes will we establish for prompt engineering and optimization? Who owns operational responsibility for uptime, error handling, and incident response? What logging and audit mechanisms will be used for GenAI inputs and outputs? How will we ensure traceability and version control for prompts and model configurations? **Risk and compliance** What processes will we establish for bias detection, mitigation, and fairness audits? What controls will be in place for data privacy, consent, and security (at rest and in transit)? How will we meet model explainability requirements, especially for high-impact or regulated use cases? How will we manage IP and copyright issues relating to AI-generated content? How will we assess and audit third-party GenAI providers for security, ethical alignment, and accountability? What safeguards will we implement against hallucination, misuse, or reputational risk in public-facing services? How will we address bias detection and adversarial robustness?  |
| IT/Data science | **Model and system architecture** What are our requirements for model transparency and explainability? How will we measure and monitor model performance over time? What are our requirements for model updates and maintenance? What level of customization do we require? What are the latency, reliability, and availability requirements for our applications? Are there specific regulatory cybersecurity requirements, such as around data residency or geographic deployment? How will the GenAI solution integrate with our existing systems?  **Vendor and ecosystem strategy** Does the vendor support open standards and interoperability (e.g., prompt formats, model interchange)? What degree of openness and transparency (e.g., open models, APIs, data) is necessary for our use case? How do open-source and proprietary models compare in terms of cost, performance, and vendor support and accountability? Are there clear exit strategies or migration paths if we outgrow a particular vendor or need to switch platforms? **Data strategy and management** How will we jointly manage data governance — including storage, audit trails, and compliance with data retention and deletion policies? Are there opportunities to collaborate with vendors on domain-specific dataset creation or fine-tuning? What processes are in place for anonymization or redaction of data before it enters GenAI systems? How does the vendor support discoverability and metadata management for both inputs and outputs? How will data flows be documented, monitored, and audited across the lifecycle? |

## 

| Implementation Objectives:  Implement and refine the GenAI tool Change management and train staff to support tool adoption and scaling  Prepare for decommissioning  |  |
| :---- | :---- |
| **Team** | **Key questions to ask** |
| Procurement | **Operational considerations** Is the vendor meeting our milestones? Are we paying the vendor in a timely manner? Do we need to issue a change order?   **Innovation and scalability** How will we support scaling what works, while allowing flexibility for departments to innovate? What structures will support knowledge sharing and solution reuse across teams or regions? How will we stay informed about GenAI capabilities and vendor offerings? |
| Agency or department buyer | **Operational considerations** How will we document and share GenAI learnings and best practices across the organization? What training and support will we offer to end users, especially in high-risk or public-facing roles? How will we test and evaluate deliverables during the development or implementation phase? Who will be responsible for these activities? **Innovation and scalability** How will we enable safe experimentation (e.g., pilots, sandboxes) while maintaining appropriate controls? How will we engage with the wider public sector, academic, or civic tech community to share best practices or co-develop tools? |
| IT/Data science | **Innovation and scalability** What frameworks will we use to evaluate, iterate on, and scale successful GenAI pilots? How can we support interoperability between different GenAI platforms or ecosystems over time? |

