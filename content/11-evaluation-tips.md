# Tips for evaluating GenAI proposals

| Who: Procurement teams What: Best practices for evaluation   |
| :---- |

Your evaluation methods will vary depending on what and how you are buying. Here are some tips to consider. 

## Universal best practices 

**Risk and Compliance**

* Ensure adherence to applicable AI regulations and standards in your jurisdiction  
* Establish explainability requirements aligned with use case criticality  
* Define data governance policies covering data sharing, retention, and deletion  
* Assess risks related to third-party dependencies and supply chains

**Lifecycle and Vendor Management**

* Plan for ongoing model maintenance, updates, and version control  
* Confirm vendor commitments to transparency about changes and potential impacts  
* Include rollback and incident response procedures in contracts

**Documentation and Governance**

* Maintain detailed records of evaluation criteria, scoring, and decisions  
* Require vendors to disclose model limitations and ethical considerations  
* Embed responsible AI requirements into procurement contracts and SLAs

**Additional Considerations** 

* Compatibility with open source GenAI models or platforms, where appropriate  
* Support for data interoperability and integration with existing public sector data repositories  
* Transparency and communication plans for public-facing GenAI applications  
* Preparedness for public records requests or transparency laws impacting AI outputs  
* Strategies to mitigate risks related to hallucinations, misinformation, or misuse

## Best practices by contract method

| Method | Best practices |
| :---- | :---- |
| **Open solicitation** | Evaluate AI-Specific Technical Criteria Suitability of the model architecture for your use case (general-purpose vs. domain-specific) Model accuracy, reliability, and robustness, including known failure modes Scalability, latency, and availability under expected workloads Security features aligned with your jurisdiction’s regulations (e.g., GDPR, HIPAA, FedRAMP) Data residency, sovereignty, and cloud region options to meet compliance needs Understand Technology and Vendor Transparency Clarity on training data provenance and governance Vendor openness about model development, fine-tuning methods, and update cadence Infrastructure dependencies and integration capabilities with your existing IT ecosystem Support for open standards and interoperability to avoid lock-in (e.g., standard APIs, model interchange formats) Review Vendor and Solution Track Record Demonstrated enterprise or government sector deployments References relevant to your region and sector Certifications and compliance attestations applicable to your regulatory environment Support for hybrid and multi-cloud strategies or on-prem deployments Pricing and Commercial Terms Transparent usage-based or subscription pricing models Availability of cost management and optimization tools Clear delineation of charges for pilot vs. production environments Contract flexibility for scaling usage or changing terms as needed Security, Privacy, and Compliance Strong data protection measures, including encryption at rest and in transit Capabilities to comply with data protection regulations (e.g., GDPR for EU, CCPA for US) Auditability and logging features for traceability Support for sovereign cloud or localized data centers where required Responsible AI and Ethical Considerations Alignment with your organization’s AI ethics framework and regulatory requirements Bias detection and mitigation strategies embedded in the solution Explainability features for AI outputs, especially where decisions impact citizens Human oversight mechanisms and escalation procedures |
| **Innovation procurement** | Include all above points, **plus:** Access to sandbox or trial environments for iterative testing Flexible contracting to support phased delivery and learning cycles Clear frameworks for intellectual property rights and data ownership Opportunities for co-development or joint innovation with providers Availability of professional services and ecosystem partnerships |
| **Framework agreement (multiple awards) or under-threshold purchase** | Focus on essential due diligence: Standardized compliance and security documentation Service Level Agreements (SLAs) with clear uptime and support commitments Transparent cost structures and usage monitoring Integration capabilities with existing systems and tools Support and escalation pathways clearly defined |

## Evaluation criteria 

For open solicitations, these are some evaluation criteria to consider. When putting these criteria into practice, you can give each criterion scoring guidance (e.g. 0, 2 for basic, 4 for advanced, 5 for ideal), and make nonzero scores mandatory for some.

| Evaluation category | Subcriteria |
| :---- | :---- |
| Problem fit and outcome alignment | Clarity of outcome goals, understanding of need, innovative solution orientation |
| AI approach and explainability | Transparency of models, algorithmic clarity, audit readiness |
| Data governance and bias handling | Strategies for fairness, data quality, addressing known limitations |
| Integration, training and ops | Fit with existing infrastructure, upskilling plans, support services |
| Risk, accountability and ethics | Liability mechanisms, ethics oversight, human control, governance structure |
| Financial value  | Cost over lifecycle, clarity of pricing, real ROI expectations |
| Vendor experience and references | GenAI experience, reference success, reliability |

