# *Solicitation* tips for AI 

```
- Who: Procurement, project, and technology teams
- What: Best practices for solicitations for AI that go beyond off-the-shelf purchases 
```

These tips can help if you are seeking to customize or build an AI solution. 

#### **As you develop your approach… {.collapsible}** 

* **Support cross-functional collaboration.** Collaborate early and often with stakeholders across the organization, including AI specialists, legal, security, compliance, and end users, to ensure the solution meets diverse needs and regulatory requirements. Have an expert who understands AI solutions on your evaluation team.  
* **Engage** **the end users and vendors early.** In line with good technology procurement, make sure that you have a deep understanding of your user or beneficiary needs through user research and testing. Equally important, the vendor market can give you helpful feedback not only on what is possible, but also what would incentivize them to participate in solicitations. Engagement efforts may include conducting a request for information (RFI), requesting demonstrations, and more.   
* **Use an outcomes-based approach.** Focus on the problem and measurable features or Key Performance Indicators (KPIs) rather than prescribing specific technologies. Consider solutions that can leverage current commercial and emerging AI capabilities while meeting your requirements. Understand how the underlying technology works, both in terms of the algorithms and training data. For example, if a health agency wants to deploy GenAI to rapidly summarize information about patients, an outcomes-based RFP might say: "we seek to reduce time/cost for feedback summaries by 50%, ensure explainability, minimize bias—all while safely handling sensitive patient data.”   
* **Support testing and iteration.** Encourage pilot projects, sandbox environments, and modular procurement to enable safe testing of AI models. Consider proof-of-concept deployments or trial periods to evaluate capabilities before full implementation. Look for documented evidence of real-world implementations in similar contexts.

#### **As you develop your solicitation documents and requirements… {.collapsible}** 

* **Incorporate compliance and risk management.** Include requirements for relevant IT security and AI-specific certifications and regulatory alignment, while balancing flexibility to encourage vendor participation. This can look like specifying any needs for bias mitigation, stakeholder participation, explainability, audit trails, human-in-the-loop controls, and transparency about training data, ensuring these align with your organization's risk management framework. Also ensure alignment with appropriate security frameworks and regional compliance requirements set out by your jurisdiction. The [NIST Risk Management Framework](https://csrc.nist.gov/projects/risk-management) can be a helpful resource.   
* **Address intellectual property (IP) and data rights.** Clearly define ownership of outputs, terms for using public sector data for model improvement, and data portability expectations. Consider contractual requirements for data residency, usage restrictions, and data handling protocols.   
* **Ensure legal and regulatory alignment.** Reference applicable AI and data protection laws in your jurisdiction and incorporate them in solicitation and contract clauses. Consider requirements for demonstrating compliance with relevant legal frameworks.  
* **Include vendor and contract management.** Plan for knowledge transfer and evaluate the long-term flexibility of your chosen solution, including data portability and integration capabilities.  
* **Specify integration requirements.** This includes existing IT environments and compliance with relevant accessibility standards, ensuring inclusiveness and usability for all users. Evaluate technical compatibility with your current systems and infrastructure. Consider data flow and interoperability requirements, and solutions that support industry-standard APIs and data formats to enable integration with your existing systems and provide flexibility for future technology evolution.  
* **Align on technical infrastructure needs for development.** Clarify whose technological infrastructure will be used for developing and deploying the solution.  
* **Include data governance and privacy requirements.** These can include data governance, breach notification protocols, and data protection responsibilities. Consider appropriate security certifications and privacy controls for your context. Understand clearly how the solution provider will use your data and address associated risks.  
* **Consider sustainability.** You may have sustainability requirements for environmental impact reporting, such as energy usage metrics and sustainability practices, aligned with your organization's environmental goals.

#### **As you draft your RFI/RFP documents, include… {.collapsible}** 

* **Core structure**: Start with your standard organizational RFI or RFP structure (e.g., timeline, instructions, scoring) and make it even stronger by focusing on the problem that you are trying to solve and the outcomes you want to achieve. Keep it short and use simple language.  
* **Technical evaluation**: Incorporate sections for AI capabilities, security measures, multi-model integration, vendor support and transitions.  
* **Quality and capability assessment**: Include questions around topics like model accuracy, hallucination, mitigation, benchmark validation, ethics, and compliance.  
* **AI disclosure**: Consider adding a requirement for transparency and accountability depending upon your policies and use case.  
* **Scoring rubric**: Assign weights to elements like security and compliance, disclosure, model performance and evaluation, support and maintenance, and pricing/value.   
* **Guarantee period:** Include a guarantee period to help ensure that the vendor will fix issues early on to meet your specifications. 

#### **In line with good procurement practices, make sure to keep your RFP long enough to enable vendors to prepare thoughtful responses, and spend time and budget to spread the word to vendors whom you might want to attract.**

```
---
icon: pin
background: green
---
### Resources for drafting solicitations

 - The [UK guidelines for AI procurement](https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement?utm_source=chatgpt.com#selection-evaluation-and-award) include a section on drafting requirements. 
 - PUBLIC has a [guide](https://view.publitas.com/public-1/buying-generative-ai-in-government/page/36-37) on buying GenAI that has specific tips for drafting requirements.
```

| Solicitation tips: Key questions \*\*Procurement and organizational objectives\*\* \- Decide on your procurement procedure \- Draft your solicitation  |  |
| :---- | :---- |
| **Team** | **Questions** |
| **Procurement** | **Project needs** \- How might we reflect the project team’s needs in a RFP?  \- Can the vendor demonstrate a roadmap and flexibility that aligns with our evolving needs? \- What level of vendor support or service guarantees will we need if something breaks or goes wrong? \- Will the vendor support integration with other AI tools or models we may adopt in the future? **Operational considerations** \- What kind of selection criteria should be used to rank solicitation responses?  \- What commercial or pricing models are appropriate? \- Are the right people in the room for this procurement? **Risk and compliance** \- How will we ensure compliance with current and evolving AI-related legislation, procurement rules, and standards? \- How will we manage and track AI-specific costs (e.g., compute, storage, API usage)? \- How much risk does vendor lock-in pose in our context, and what mitigations are practical? |
| **Agency or department buyer** | **AI solution** \- Do we know what “good enough” model response performance looks like for our specific use case, and what metrics will we use to measure AI success and ROI (qualitative and quantitative)? \- What are our fallback strategies if the model gives low-quality or inaccurate responses? \- How will users interact with the AI solution? If there is a platform or dashboard, who is responsible for developing it? \- Can we retrieve or export our data and models if we switch vendors?  **Data strategy and management** \- Do we and our vendor partners understand the quality, completeness, and relevance of our data? \- What role will the vendor play in helping us prepare or transform data for effective use? \- How will we manage data ownership, licensing, and usage rights?  \- What safeguards will the vendor provide to ensure that sensitive, personal, or classified data is handled in line with legal and policy requirements?  \- Will the AI system retain or learn from our data inputs? Under what terms, and with what controls? \- Is the algorithm specialized for our organization? If so, will we have input into how the algorithm is fine-tuned or retrained using our data? What additional data sources do we need to get the best results?  **Operational considerations** \- How might we approach AI implementation in stages, such as through a pilot or sandbox? \- How will we measure and monitor model performance over time? \- What policies will govern human-in-the-loop oversight for critical decision-making? \- What mechanisms will we put in place for continuous performance monitoring and system health checks? \- What accident response mechanisms will we put in place? What processes will we follow to learn from accidents and improve their prevention and the management of associated risks and harms? \- If we are involved in retraining or fine-tuning the model, what internal processes will we establish for prompt engineering and optimization? \- Who owns operational responsibility for uptime, error handling, and incident response? \- What logging and audit mechanisms will be used for AI inputs and outputs? \- How will we ensure traceability and version control for prompts and model configurations? **Risk and compliance** \- What processes will we establish for bias detection, mitigation, and fairness audits? \- What controls will be in place for data privacy, consent, and security (at rest and in transit)? \- How will we meet model explainability requirements, especially for high-impact or regulated use cases? \- How will we manage IP and copyright issues relating to AI-generated content? \- How will we assess and audit third-party AI providers for security, ethical alignment, and accountability? \- What safeguards will we implement against hallucination, misuse, or reputational risk in public-facing services? \- How will we address bias detection and adversarial robustness?  |
| **IT/Data analytics** | **Project needs** \- What are our requirements for model transparency and explainability? \- How will we measure and monitor model performance over time? \- What are our requirements for model updates and maintenance? \- What level of customization do we require? \- What are the latency, reliability, and availability requirements for our applications? \- Are there specific regulatory cybersecurity requirements, such as around data residency or geographic deployment? **Integration** \- How will the AI solution integrate with our existing systems?  \- Does the vendor support open standards and interoperability (e.g., prompt formats, model interchange)? \- How do open-source and proprietary models compare in terms of cost, performance, and vendor support and accountability? \- Are there clear exit strategies or migration paths if we outgrow a particular vendor or need to switch platforms? **Data strategy and management** \- How will we jointly manage data governance – including storage, audit trails, and compliance with data retention and deletion policies? \- Are there opportunities to collaborate with vendors on domain-specific dataset creation or fine-tuning? \- What processes are in place for anonymization or redaction of data before it enters AI systems? \- How does the vendor support discoverability and metadata management for both inputs and outputs? \- How will data flows be documented, monitored, and audited across the lifecycle? |

